## (10p) Pipelining
### (1p) Why is it important to design a pipeline so that the different pipeline stages need approximately the same time to perform their work?
### Answer:
The clock cycle is tied to how difficult each of the steps in execution process takes. If one of the stages in the pipeline takes longer time the other parts has to wait for that part to finish, even if they were done a long time ago. If all parts take the same amount of time a new instruction can be fetched, decoded, and executed each clock cycle. 
### (3p) Consider a simple 5-stage pipelined RISC processor. Which pipeline stages are useful and what does each of them do?
### Answer:
1. Instruction Fetch - Fetches the next instruction pointed to by the PC. Increment the PC by four.
2. Instruction Decode - Decode the instruction and fetch the relevant registers and / or the constants.
3. Execute - Execute the instruction.
4. Memory access - If the instruction tells us to access the memory for read or write do that.
5. Write back - The instruction is written back to the register present in the instruction.

### (5p) Explain what is needed in a superscalar processor to be able to effciently execute multiple instructions each clock cycle?
### Answer:
For a superscalar processor to work both efficiently the following data structures need to be implemented into hardware:
+ Branch prediction: Being able to speculatively execute instructions in a branch (i.e. conditional jump) that may or may not be executed. This is avoid stalling when the program reaches such a branch. To help with this there is branch history table that is continuesly updated depending on the correctness of the previous prediciton and will take into account the previous predictions when encountering the branch in the future. To handle any incorrect speculative executions it also needs the following two components:

+ Reorder buffer:
A reorder buffer is a FIFO queue where the oldest instruction gets executed first, if we say execute a conditonal jump and it gives a result that is not what the branch prediction "thought" we should have executed, it is easy to remove all instructions in the buffer and load in the correct instructions.

+ Rename registers: There are a limited amount of registers available and if data uses the same registers for multiple instructions the pipeline can stall whilst waiting for that register to be available. Rename registers uses a table to give a "synonym" for the register an instructions wishes to write to and instead writes it there. When another instructions wishes to read data from a register it checks wheter the latest value is in the register and uses that if it is, otherwise it takes the value from the rename register that register has and instead uses that value if it is available, or waits for it to be. This removes output and anti-dependendencies from using the same registers and can substantially speed up the execution.  


## (10p) Cache Memories
### (4p) Explain what temporal and spatial locality means. Give examples when this can be exploited when executing a program.
### Answer:
+ Temporal locality:
Through obeservation we can see that when an address is used, it is likely to be used again somewhere close in the future. Therefore it is more efficient to store it in the cache for faster fetching of the data. Example, we have `for`-loop that uses a value or instruction that is to large to fit in a register and is used in every iteration of the loop, then it is a stored in the cache for faster access.

+ Spatial locality:
Through observation we can see that when an address, _A_, is accessed it is likely that the address _A + 1_ is also going to be used. This is done by instead of loading each address for every cache miss, we also a set size of the following addresses into the cache. Exploiting this can be done for example when doing mulitple operations on an array, such as adding 1 to each element, since more than just the first address of element is loaded into the cache we get more performance from the cache since the next element can be accessed from the cache even though it might not have been encountered specifically in the past.

### (2p) What is the purpose of a having an _N_-way associative cache instead of a direct mapped cache? What is the additional hardware cost?
### Answer:
In a direct mapped cache there is higher probabilty of a two addresses used frequently in a function to push each other out of the cache everytime the address is accessed. With an _N_-way associative cache there are _N_ different rows that each address can be written to, allowing for better performance compared, which has the cost of _N-1_ comparators having to be constructed on the chip compared to a direct mapped cache.

### (4p) In C programs with nested 
